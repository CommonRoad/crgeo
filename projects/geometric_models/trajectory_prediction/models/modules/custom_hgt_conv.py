import math
from typing import Callable, Sequence
from typing import List, Dict, Tuple, Optional, Union, Literal, TypeVar, Iterable

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Parameter, LayerNorm, ModuleDict, ParameterDict
from torch_sparse import SparseTensor

from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.dense import Linear
from torch_geometric.nn.inits import glorot, ones, reset
from torch_geometric.typing import EdgeType, Metadata, NodeType
from torch_geometric.utils import softmax, add_self_loops, contains_self_loops

from commonroad_geometric.common.type_checking import all_same

K = TypeVar('K')
V = TypeVar("V")


def zip_dict_items(*dicts: Dict[K, V]) -> Iterable[Tuple[K, V]]:
    if len(dicts) > 0:
        for key in dicts[0].keys():
            yield key, *(d[key] for d in dicts)


ActivationFunction = Callable[[Tensor], Tensor]

class CustomHGTConv(MessagePassing):

    GLOBAL_CONTEXT_NODE: NodeType = "global context"

    r"""
    Args:
        in_channels (int or Dict[str, int]): Size of each input sample of every
            node type, or :obj:`-1` to derive the size from the first input(s)
            to the forward method.
        out_channels (int): Size of each output sample.
        metadata (Tuple[List[str], List[Tuple[str, str, str]]]): The metadata
            of the heterogeneous graph, *i.e.* its node and edge types given
            by a list of strings and a list of string triplets, respectively.
            See :meth:`torch_geometric.data.HeteroData.metadata` for more
            information.
        attention_heads (int, optional): Number of multi-head-attentions.
            (default: :obj:`1`)
        aggregation (string, optional): The aggregation scheme to use for grouping
            node embeddings generated by different relations.

            (:obj:`"sum"`, :obj:`"mean"`, :obj:`"min"`, :obj:`"max"`).
            :obj:`torch.{aggregation}`
            default: :obj:`"sum"`
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """
    def __init__(
        self,
        in_channels_node: Dict[NodeType, int],
        in_channels_edge: Dict[EdgeType, int],
        attention_channels: int,
        out_channels_node: Dict[NodeType, int],
        metadata: Metadata,
        attention_heads: int,
        neighbor_aggregation: Literal["sum", "mean", "min", "max", "mul", None],
        aggregation: Literal["sum", "mean", "min", "max", "mul", None],
        activation_fn: ActivationFunction = F.gelu,
        meta_relation_prior: bool = True,
        residual_connection: bool = True,
        add_self_loops: Optional[Sequence[EdgeType]] = None,
        global_context: bool = True,
        **kwargs,
    ):
        super().__init__(aggr=neighbor_aggregation, node_dim=0, **kwargs)
        node_types, edge_types = metadata

        if global_context:
            assert self.GLOBAL_CONTEXT_NODE in in_channels_node and self.GLOBAL_CONTEXT_NODE in out_channels_node

            self.global_in_channels = in_channels_node[self.GLOBAL_CONTEXT_NODE]
            self.global_out_channels = out_channels_node[self.GLOBAL_CONTEXT_NODE]

            in_channels_node = in_channels_node.copy()
            out_channels_node = out_channels_node.copy()
            del in_channels_node[self.GLOBAL_CONTEXT_NODE]
            del out_channels_node[self.GLOBAL_CONTEXT_NODE]

            # force all node feature vectors to be of the same size so they can be added up for
            # updating the global context vector
            assert all_same(out_channels_node.values())

        assert set(in_channels_node.keys()) == set(out_channels_node.keys()) == set(node_types)
        assert set(in_channels_edge.keys()) == set(edge_types)
        assert add_self_loops is None or len(add_self_loops) > 0 and all(src == dst for (src, _, dst) in add_self_loops)

        self.in_channels_node: Dict[NodeType, int] = in_channels_node
        self.in_channels_edge: Dict[EdgeType, int] = in_channels_edge
        self.attention_channels: int = attention_channels
        self.out_channels_node: Dict[NodeType, int] = out_channels_node
        assert all(out_channels % attention_heads == 0 for out_channels in out_channels_node.values())

        self.attention_heads = attention_heads
        self.aggregation = aggregation
        self.activation_fn = activation_fn
        self.meta_relation_prior = meta_relation_prior
        self.residual_connection = residual_connection
        self.add_self_loops = add_self_loops
        self.global_context = global_context

        if residual_connection:
            assert all(out_channels_node[node_type] == in_channels_node[node_type] for node_type in node_types), \
                "Output channels have to match input channels for all node types if residual connection is enabled"

        assert attention_channels % attention_heads == 0
        dim_attention = attention_channels // attention_heads

        # node-type-specific parameters
        self.k_node_lin = ModuleDict()
        self.q_node_lin = ModuleDict()
        self.v_node_lin = ModuleDict()
        self.message_layer_norm = ModuleDict()
        self.aggr_lin = ModuleDict()
        self.residual_weight = ParameterDict()  # beta
        self.residual_layer_norm = ModuleDict()
        for node_type, in_channels, out_channels in ((t, self.in_channels_node[t], self.out_channels_node[t]) for t in node_types):
            # matrices for all attention heads combined
            # attention
            self.k_node_lin[node_type] = Linear(in_channels, attention_channels)
            self.q_node_lin[node_type] = Linear(in_channels, attention_channels)
            # message
            self.v_node_lin[node_type] = Linear(in_channels, out_channels)
            self.message_layer_norm[node_type] = LayerNorm([out_channels])
            # update
            self.aggr_lin[node_type] = Linear(out_channels, out_channels)
            self.residual_weight[node_type] = Parameter(torch.Tensor(1))
            self.residual_layer_norm[node_type] = LayerNorm([out_channels])

        # edge-type-specific parameters
        self.k_edge_lin = ModuleDict()
        self.q_edge_lin = ModuleDict()
        self.att_meta_lin = ParameterDict()
        self.v_meta_node_lin = ModuleDict()
        self.v_meta_edge_lin = ModuleDict()
        self.prior = ParameterDict()
        for edge_type in edge_types:
            src_type, _, dst_type = edge_type
            dim_node = self.out_channels_node[src_type]
            dim_out = self.out_channels_node[dst_type]
            dim_edge = self.in_channels_edge[edge_type]
            meta_relation = "__".join(edge_type)
            # attention
            self.k_edge_lin[meta_relation] = Linear(dim_edge, attention_channels)
            self.q_edge_lin[meta_relation] = Linear(dim_edge, attention_channels)
            self.att_meta_lin[meta_relation] = Parameter(torch.Tensor(attention_heads, dim_attention, dim_attention))
            self.prior[meta_relation] = Parameter(torch.Tensor(attention_heads))
            # message
            self.v_meta_node_lin[meta_relation] = Linear(dim_node, dim_out)
            self.v_meta_edge_lin[meta_relation] = Linear(dim_edge, dim_out)

        # parameters for global context
        self.global_lin: Optional[Linear] = None
        if global_context:
            dim_out = next(iter(out_channels_node.values()))
            self.global_lin = Linear(dim_out, self.global_out_channels)
            self.q_node_lin[self.GLOBAL_CONTEXT_NODE] = Linear(self.global_in_channels, attention_channels)

        self.reset_parameters()

    def reset_parameters(self):
        # reset(m) calls m.reset_parameters() if it exists, otherwise it resets the modules' children
        reset(self.k_node_lin)
        reset(self.q_node_lin)
        reset(self.v_node_lin)
        reset(self.message_layer_norm)
        reset(self.aggr_lin)
        ones(self.residual_weight)
        reset(self.residual_layer_norm)
        ones(self.prior)
        reset(self.k_edge_lin)
        reset(self.q_edge_lin)
        glorot(self.att_meta_lin)
        reset(self.v_meta_node_lin)
        reset(self.v_meta_edge_lin)
        if self.global_lin is not None:
            reset(self.global_lin)

    def forward(
        self,
        x_dict: Dict[NodeType, Tensor],
        edge_index_dict: Union[Dict[EdgeType, Tensor], Dict[EdgeType, SparseTensor]],  # Support both.
        edge_attr_dict: Dict[EdgeType, Tensor],
    ) -> Dict[NodeType, Optional[Tensor]]:
        r"""
        Args:
            x_dict (Dict[str, Tensor]): A dictionary holding input node
                features  for each individual node type.
            edge_index_dict: (Dict[str, Union[Tensor, SparseTensor]]): A
                dictionary holding graph connectivity information for each
                individual edge type, either as a :obj:`torch.LongTensor` of
                shape :obj:`[2, num_edges]` or a
                :obj:`torch_sparse.SparseTensor`.

        :rtype: :obj:`Dict[str, Optional[Tensor]]` - The output node embeddings
            for each node type.
            In case a node type does not receive any message, its output will
            be set to :obj:`None`.
        """
        H, D_head = self.attention_heads, self.attention_channels // self.attention_heads

        k_node_dict: Dict[NodeType, Tensor] = {}
        q_node_dict: Dict[NodeType, Tensor] = {}
        v_node_dict: Dict[NodeType, Tensor] = {}
        out_aggr_dict: Dict[NodeType, List[Tensor]] = {}

        node_types = set(x_dict.keys())

        if self.global_context:
            if self.GLOBAL_CONTEXT_NODE in x_dict:
                node_types.remove(self.GLOBAL_CONTEXT_NODE)
            else:
                # add context vector of all 0's if it does not exist in the input features
                x_dict[self.GLOBAL_CONTEXT_NODE] = torch.zeros(
                    (self.global_in_channels,),
                    dtype=torch.float,
                    device=next(self.parameters()).data.device,
                )

        # Compute the node part of attention vectors: key, query, value
        for node_type, x in ((node_type, x_dict[node_type]) for node_type in node_types):
            k_node_dict[node_type] = self.k_node_lin[node_type](x).view(-1, H, D_head)
            q_node_dict[node_type] = self.q_node_lin[node_type](x).view(-1, H, D_head)
            v_node_dict[node_type] = self.v_node_lin[node_type](x)
            out_aggr_dict[node_type] = []

        if self.global_context:
            q_node_dict[self.GLOBAL_CONTEXT_NODE] = self.q_node_lin[self.GLOBAL_CONTEXT_NODE](x_dict[self.GLOBAL_CONTEXT_NODE])

        # Iterate over edge-types / meta relations
        for edge_type, edge_index, edge_attr in zip_dict_items(edge_index_dict, edge_attr_dict):
            src_type, _, dst_type = edge_type
            meta_relation = "__".join(edge_type)
            D_in_edge = self.in_channels_edge[edge_type]
            D_value = self.out_channels_node[dst_type] // self.attention_heads

            # N x H x D_head
            k_node = k_node_dict[src_type]
            q_node = q_node_dict[dst_type]

            # N x H x D_value
            v_node = self.v_meta_node_lin[meta_relation](v_node_dict[src_type]).view(-1, H, D_value)

            # Compute the edge part of attention vectors: key, query, value
            if D_in_edge == 0:
                # no edge attributes
                k_edge = torch.zeros((edge_attr.size(0), H, D_head), dtype=edge_attr.dtype, device=edge_attr.device)
                q_edge = torch.zeros((edge_attr.size(0), H, D_head), dtype=edge_attr.dtype, device=edge_attr.device)
                v_edge = torch.zeros((edge_attr.size(0), H, D_value), dtype=edge_attr.dtype, device=edge_attr.device)
            else:
                # E x H x D_head
                k_edge = self.k_edge_lin[meta_relation](edge_attr).view(-1, H, D_head)
                q_edge = self.q_edge_lin[meta_relation](edge_attr).view(-1, H, D_head)
                # E x H x D_value
                v_edge = self.v_meta_edge_lin[meta_relation](edge_attr).view(-1, H, D_value)

            # prior
            if self.meta_relation_prior:
                prior = self.prior[meta_relation]
            else:
                prior = torch.ones((self.attention_heads,), dtype=torch.float)

            k_node_self_loop = None
            q_node_self_loop = None
            v_node_self_loop = None
            if self.add_self_loops is not None and edge_type in self.add_self_loops:
                k_node_self_loop = k_node_dict[dst_type]
                q_node_self_loop = q_node_dict[dst_type]
                v_node_self_loop = self.v_meta_node_lin[meta_relation](v_node_dict[dst_type]).view(-1, H, D_value)

                assert not contains_self_loops(edge_index)
                # self-loop edges are appended at the end of the original edge_index
                edge_index, _ = add_self_loops(edge_index)

            out = self.propagate(
                edge_index,
                k_node=k_node,
                k_node_self_loop=k_node_self_loop,
                k_edge=k_edge,
                att_lin=self.att_meta_lin[meta_relation],
                q_node=q_node,
                q_node_self_loop=q_node_self_loop,
                q_edge=q_edge,
                v_node=v_node,
                v_node_self_loop=v_node_self_loop,
                v_edge=v_edge,
                prior=prior,
                size=None,
            )  # N x D_out

            # apply layer normalization on the messages
            out = self.message_layer_norm[dst_type](out)

            out_aggr_dict[dst_type].append(out)

        # aggregate node feature vectors
        out_dict: Dict[NodeType, Optional[Tensor]] = {}
        for node_type, outs in out_aggr_dict.items():
            out = self._group(outs)

            if out is None:
                out_dict[node_type] = None
                continue

            # apply non-linearity
            # then map aggregated feature vector to node-type-specific feature distribution of the target node
            out = self.aggr_lin[node_type](self.activation_fn(out))

            # residual connection
            if self.residual_connection:
                assert out.size() == x_dict[node_type].size()
                beta = self.residual_weight[node_type].sigmoid()
                out = beta * out + (1 - beta) * x_dict[node_type]
                out = self.residual_layer_norm[node_type](out)

            out_dict[node_type] = out

        # update global context
        if self.global_context:
            alpha_list, message_list = [], []
            q = self.q_node_lin[self.GLOBAL_CONTEXT_NODE](x_dict[self.GLOBAL_CONTEXT_NODE])
            sqrt_d = math.sqrt(self.attention_channels)
            for node_type in node_types:
                h = out_dict[node_type]
                k = self.k_node_lin[node_type](h)
                a = (q * k).sum(dim=-1) / sqrt_d
                alpha_list.append(a)

                message_list.append(self.v_node_lin[node_type](h))

            alpha = torch.cat(alpha_list, dim=0)
            alpha = F.softmax(alpha, dim=0)

            message_global = torch.sum(alpha.view(-1, 1) * torch.cat(message_list, dim=0), dim=0)
            out_dict[self.GLOBAL_CONTEXT_NODE] = self.global_lin(self.activation_fn(message_global))

        return out_dict

    def _group(self, xs: List[Tensor]) -> Optional[Tensor]:
        if len(xs) == 0:
            return None
        elif self.aggregation is None:
            return torch.stack(xs, dim=1)
        elif len(xs) == 1:
            return xs[0]
        else:
            aggregation_fn = getattr(torch, self.aggregation)
            out = torch.stack(xs, dim=0)
            out = aggregation_fn(out, dim=0)
            out = out[0] if isinstance(out, tuple) else out
            return out

    # _i = target, _j = source
    # source -> target

    # E = #edges
    # H = #attention heads
    # D_head = dimension of each attention head

    # noinspection PyMethodOverriding
    def message(
        self,
        k_node_j: Tensor,  # source, E x H x D_head
        k_node_self_loop_j: Optional[Tensor],  # source, E x H x D_head
        k_edge: Tensor,  # E_orig x H x D_head
        att_lin: Tensor,  # H x D_head x D_head
        q_node_i: Tensor,  # target, E x H x D_head
        q_node_self_loop_i: Optional[Tensor],  # target, E x H x D_head
        q_edge: Tensor,  # E x H x D_head
        v_node_j: Tensor,  # source, E x H x D_value
        v_node_self_loop_j: Optional[Tensor],  # source, E x H x D_value
        v_edge: Tensor,  # E x H x D_value
        prior: Tensor,  # H
        edge_index_i: Tensor,  # E
        ptr: Optional[Tensor],  # non-null when using sparse tensors for the edge index
        size_i: Optional[int],
    ) -> Tensor:
        E_orig = k_edge.size(0)  # number of edges in the original graph without self-loops
        has_self_loops = E_orig != k_node_j.size(0)

        # E x H x D_head
        if has_self_loops:
            k_j = torch.empty_like(k_node_j)
            k_j[:E_orig] = k_node_j[:E_orig] + k_edge
            k_j[E_orig:] = k_node_self_loop_j[E_orig:]  # self-loops
        else:
            k_j = k_node_j + k_edge

        #  H x E x D_head @ H x D_head x D_head -> H x E x D_head -> E x H x D_head
        k_meta_j = (k_j.transpose(0, 1) @ att_lin).transpose(1, 0)

        # E x H x D_head
        if has_self_loops:
            q_i = torch.empty_like(q_node_i)
            q_i[:E_orig] = q_node_i[:E_orig] + q_edge
            q_i[E_orig:] = q_node_self_loop_i[E_orig:]  # self-loops
        else:
            q_i = q_node_i + q_edge

        # E x H x D_value
        if has_self_loops:
            v_j = torch.empty_like(v_node_j)
            v_j[:E_orig] = v_node_j[:E_orig] + v_edge
            v_j[E_orig:] = v_node_self_loop_j[E_orig:]  # self-loops
        else:
            v_j = v_node_j + v_edge

        D_head = q_node_i.size(-1)
        scores = (k_meta_j * q_i).sum(dim=-1) * (prior / math.sqrt(D_head))  # E x H
        # softmax over attention scores of the current meta relation
        scores = softmax(scores, edge_index_i, ptr, size_i)
        out = v_j * scores.unsqueeze(-1)  # E x H x D_value
        D_out = out.size(-2) * out.size(-1)  # D_out = H * D_value
        # "stack" the vectors of all the attention heads:
        return out.view(-1, D_out)  # E x H*D_value = E x D_out
